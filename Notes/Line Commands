IMPORTANT LINUX COMMANDS For HDFS:
Find . -name “*searchterm*” # The . means current folder, change searchterm.
!quit # Exits you from the beeline
beeline -u jdbc:hive2:// # Start a beeline.
sudo nano .bashrc # Open .bashrc from home to edit then just exit with ctrl+x.
sudo ssh restart
sudo service ssh restart
sbin/start-dfs.sh # Start to start (Need to be in $HADOOP_HOME
sbin/stop-yarn.sh # Stop to stop
jps # Check if it works
wsl --shutdown # From powershell

sudo nano .bashrc
htop
cat > movemepls
jps
wsl --shutdown
rm moveme # Deletes
HADOOP:
bin/hdfs dfs -touchz  /jordi/testing2.txt 
hadoop fs -mkdir -p /jordi/testing2
----------------------------------------
hadoop fs -put moveme /jordi/
hdfs dfs -ls /jordi/                      # Move
hadoop fs -put MortalityDatabase /user/hive/warehouse/ # Move from linux memory to hive warehouse.
----------------------------------------
hdfs dfs -rmdir /jordi
---------------------------------------------------------------------------------------------------------
==========================================================================================================
Important Commands P2 Using Beeline:
hdfs dfs -ls /user/hive/warehouse/dem         # Check to see if the demonstration worked.
beeline -u jdbc:hive2://                      # Enter hive as specific user.

./spark-shell   # Do this once you are in hadoop/spark/bin.
